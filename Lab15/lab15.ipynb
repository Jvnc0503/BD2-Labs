{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ecc817",
   "metadata": {},
   "source": [
    "# Lab15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248124a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Consultas requeridas\n",
    "1. Obtener todas las mediciones de humedad del sensor 'SENS001' del último día. La consulta debe incluir el tiempo restante de vida (TTL) de cada registro.\n",
    "\n",
    "2. Detectar valores anómalos fuera del rango permitido en la última hora. Implementar una consulta que identifique mediciones de temperatura o humedad fuera del rango normal. La consulta debe permitir filtrar por sensor específico.\n",
    "\n",
    "3. Verificar el tiempo restante de vida de los datos usando la función TTL. Implementar una consulta que muestre el TTL en diferentes unidades (segundos, horas, días). Crear una consulta para identificar datos que están próximos a expirar (ej: en las próximas 24 horas).\n",
    "\n",
    "## Estructura de tabla propuesta\n",
    "La tabla *sensor_readings* tendrá un propósito general que permite filtrar por tipo de medicion, sensor y día. Esta tabla se usará para la consulta 1.\n",
    "\n",
    "La tabla *sensor_anomalies* será destinada a optimizar la consulta 2, esta nos permite filtrar por hora ya que se incluye este dato en el partition key, además de tener clustering por el valor de la medición lo que facilita hallar los valores anómalos.\n",
    "\n",
    "La tabla *sensor_by_date* fue pensada para usarse con la consulta 3, porque particiona solo por la fecha y con esto poder filtrar los datos con más de 6 días de antigüedad.\n",
    "\n",
    "Se respeta el tiempo de vida de los datos de 7 días de acuerdo a lo solicitado, y se considera adicionalmente un tiempo de vida de solo 2 horas para el seguimiento de datos anómalos ya que se espera que siempre se consulten los insertados en la hora pasada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811eb3c",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create table if not exists sensor_readings\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, date), event_time )\n",
    ") with clustering order by (event_time desc) and\n",
    "        default_time_to_live = 604800; -- 7 days\n",
    "\n",
    "create table if not exists sensor_anomalies\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    hour             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, hour), measurement, event_time)\n",
    ") with clustering order by (measurement asc, event_time desc) and\n",
    "        default_time_to_live = 7200; -- 2 hour\n",
    "\n",
    "create table if not exists sensor_by_date\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( date, event_time )\n",
    ") with clustering order by (event_time desc) and\n",
    "        default_time_to_live = 604800; -- 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e96cc4",
   "metadata": {},
   "source": [
    "## Creación de datos de prueba\n",
    "\n",
    "Instalación de driver, se recomienda correr en un enviroment de conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58fbb15",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (3.29.2)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: click in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b15792",
   "metadata": {},
   "source": [
    "Script en python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e4b376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserción de datos de prueba completada.\n",
      "Total de registros insertados: 100810\n",
      "\n",
      "Query 1:\n",
      "Lecturas de temperatura para el sensor SENS001 del día de ayer 2025-07-07:\n",
      "1440 resultados encontrados en 0.0496 segundos\n",
      "  - 2025-07-07 23:59:03: 23.996825977300986 (TTL: 604788 segundos)\n",
      "  - 2025-07-07 23:58:03: 18.05019900240751 (TTL: 604788 segundos)\n",
      "  - 2025-07-07 23:57:03: 22.191578415285527 (TTL: 604788 segundos)\n",
      "  - 2025-07-07 23:56:03: 20.917350945396255 (TTL: 604788 segundos)\n",
      "  - 2025-07-07 23:55:03: 17.66867800150658 (TTL: 604788 segundos)\n",
      "\n",
      "Query 2:\n",
      "Anomalías de humedad para el sensor SENS002 de la hora pasada 2025-07-08T15:\n",
      "19 resultados encontrados en 0.0274 segundos\n",
      "  - 2025-07-08 15:59:03: 20.99336421314097\n",
      "  - 2025-07-08 15:56:03: 87.11390164108695\n",
      "  - 2025-07-08 15:54:03: 20.04462670966518\n",
      "  - 2025-07-08 15:52:03: 84.68263384130653\n",
      "  - 2025-07-08 15:50:03: 85.11937172743727\n",
      "\n",
      "Query 3:\n",
      "Datos de más de 6 días de antigüedad (2025-07-02):\n",
      "1440 resultados encontrados en 0.0603 segundos\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604797 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604797 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604797 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604797 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604797 segundos, 10079 minutos, 167 horas, 6 días)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement, BatchStatement, BatchType\n",
    "\n",
    "# Parametros de simulación\n",
    "N_SENSORS = 5\n",
    "SAMPLING_RATE = 60 # 1 minuto en segundos\n",
    "SAMPLING_TIME = 604800  # 7 días en segundos\n",
    "ID_PREFIX = 'SENS'\n",
    "MESUREMENTS_TYPES = ('temperatura', 'humedad')\n",
    "NORMAL_RANGE = {'temperatura': (15, 35),\n",
    "                'humedad': (30, 80)}\n",
    "\n",
    "# Precomputar rango extendido de valores para simulación de anomalías\n",
    "EXTENDED_RANGE = {\n",
    "    type: (low - (high - low) * 0.2,\n",
    "        high + (high - low) * 0.2\n",
    "    )\n",
    "    for type, (low, high) in NORMAL_RANGE.items()\n",
    "}\n",
    "\n",
    "# Conexión a Cassandra en Docker\n",
    "cluster = Cluster(\n",
    "  ['localhost'], port=9042,\n",
    "  protocol_version=4,\n",
    "  connect_timeout=5,\n",
    "  idle_heartbeat_interval=30,\n",
    "  control_connection_timeout=10\n",
    ")\n",
    "cassandra = cluster.connect('my_keyspace')\n",
    "\n",
    "def create_tables() -> None:\n",
    "    # Crear tabla para lecturas de sensores\n",
    "    cassandra.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS sensor_readings (\n",
    "        measurement_type text,\n",
    "        sensor_id        text,\n",
    "        date             text,\n",
    "        event_time       timestamp,\n",
    "        measurement      double,\n",
    "        PRIMARY KEY ((measurement_type, sensor_id, date), event_time)\n",
    "      ) WITH CLUSTERING ORDER BY (event_time DESC)\n",
    "        AND default_time_to_live = 604800;\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS sensor_anomalies (\n",
    "        measurement_type text,\n",
    "        sensor_id        text,\n",
    "        hour             text,\n",
    "        event_time       timestamp,\n",
    "        measurement      double,\n",
    "        PRIMARY KEY ((measurement_type, sensor_id, hour), measurement, event_time)\n",
    "      ) WITH CLUSTERING ORDER BY (measurement ASC, event_time DESC)\n",
    "        AND default_time_to_live = 7200;\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS sensor_by_date (\n",
    "        measurement_type text,\n",
    "        sensor_id        text,\n",
    "        date             text,\n",
    "        event_time       timestamp,\n",
    "        measurement      double,\n",
    "        PRIMARY KEY (date, event_time)\n",
    "      ) WITH CLUSTERING ORDER BY (event_time DESC)\n",
    "        AND default_time_to_live = 604800;\n",
    "    \"\"\")\n",
    "\n",
    "def drop_tables() -> None:\n",
    "    # Eliminar tablas si existen\n",
    "    cassandra.execute(\"drop table if exists sensor_readings\")\n",
    "    cassandra.execute(\"drop table if exists sensor_anomalies\")\n",
    "    cassandra.execute(\"drop table if exists sensor_by_date\")\n",
    "    \n",
    "def generate_sensor_data() -> None:\n",
    "    # Generar indetificadores únicos para cada sensor\n",
    "    ids: list[str] = [f\"{ID_PREFIX}{str(i).zfill(3)}\" for i in range(1, N_SENSORS + 1)]\n",
    "\n",
    "    # Preparar queries\n",
    "    insert_reading = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_readings (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_anomaly = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_anomalies (measurement_type, sensor_id, hour, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_by_date = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_by_date (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    # Definir el tiempo de inicio y fin para la generación de datos\n",
    "    start_time: datetime = datetime.now() - timedelta(seconds=SAMPLING_TIME)\n",
    "    end_time: datetime = datetime.now()\n",
    "    current_time: datetime = end_time   # Insertar en orden descendente\n",
    "\n",
    "    # Usar BatchStatement para agrupar inserciones\n",
    "    batch = BatchStatement(batch_type=BatchType.UNLOGGED)\n",
    "\n",
    "    # Lista para almacenar las futuras ejecuciones asíncronas\n",
    "    futures = []\n",
    "\n",
    "    # Limite de inflight para evitar sobrecargar Cassandra\n",
    "    max_inflight = 16\n",
    "\n",
    "    while current_time >= start_time:\n",
    "        date = current_time.strftime('%Y-%m-%d')\n",
    "        hour = current_time.strftime('%Y-%m-%dT%H')\n",
    "\n",
    "        for id in ids:\n",
    "            for type in MESUREMENTS_TYPES:\n",
    "                ext_low, ext_high = EXTENDED_RANGE[type]\n",
    "                measurement = random.uniform(ext_low, ext_high)\n",
    "\n",
    "                batch.add(insert_reading, (type, id, date, current_time, measurement))\n",
    "                batch.add(insert_anomaly, (type, id, hour, current_time, measurement))\n",
    "                batch.add(insert_by_date, (type, id, date, current_time, measurement))\n",
    "\n",
    "        # Ejecutar el batch de forma asíncrona\n",
    "        futures.append(cassandra.execute_async(batch))\n",
    "        batch.clear()  # Limpiar el batch para la siguiente iteración\n",
    "\n",
    "        # Si el batch alcanza el límite de inflight, esperar a que se completen\n",
    "        if len(futures) >= max_inflight:\n",
    "            for f in futures:\n",
    "                f.result()\n",
    "            futures.clear()\n",
    "\n",
    "        # Retroceder el tiempo para la siguiente iteración\n",
    "        current_time -= timedelta(seconds=SAMPLING_RATE)\n",
    "    \n",
    "    # Esperar batchs restantes\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "    result = cassandra.execute(\"select count(*) from sensor_readings\")\n",
    "    count = list(result)[0][0]\n",
    "    print(\"Inserción de datos de prueba completada.\\nTotal de registros insertados:\", count)\n",
    "\n",
    "def query_1(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 1:\")\n",
    "    # Calcular el bucket del día anterior\n",
    "    target_date: str = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement, ttl(measurement) as ttl\n",
    "        from sensor_readings\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and date = %s\n",
    "    \"\"\", (type, id, target_date))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Lecturas de {type} para el sensor {id} del día de ayer {target_date}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        ttl_seconds = row.ttl\n",
    "        print(f\"  - {event_time}: {measurement} (TTL: {ttl_seconds} segundos)\")\n",
    "\n",
    "def query_2(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 2:\")\n",
    "    # Calcular el bucket de la hora anterior\n",
    "    target_hour: str = (datetime.now() - timedelta(hours=1)).strftime('%Y-%m-%dT%H')\n",
    "    # Obtener el rango normal para el tipo de medición\n",
    "    low, high = NORMAL_RANGE[type]\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows_low = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement < %s\n",
    "    \"\"\", (type, id, target_hour, low))\n",
    "\n",
    "    rows_high = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement > %s\n",
    "    \"\"\", (type, id, target_hour, high))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows_low) + list(rows_high)\n",
    "    rows.sort(key=lambda row: row.event_time, reverse=True)\n",
    "    print(f\"Anomalías de {type} para el sensor {id} de la hora pasada {target_hour}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 anomalías\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        print(f\"  - {event_time}: {measurement}\")\n",
    "\n",
    "def query_3() -> None:\n",
    "    print(\"\\nQuery 3:\")\n",
    "    # Calcular el bucket de la fecha de hace 6 días\n",
    "    target_date: str = (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select measurement_type, sensor_id, date, event_time, ttl(measurement) as ttl\n",
    "        from sensor_by_date\n",
    "        where date = %s\n",
    "    \"\"\", (target_date,))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Datos de más de 6 días de antigüedad ({target_date}):\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        measurement_type = row.measurement_type\n",
    "        sensor_id = row.sensor_id\n",
    "        date = row.date\n",
    "        ttl_seconds = row.ttl\n",
    "        ttl_minutes = ttl_seconds // 60\n",
    "        ttl_hours = ttl_minutes // 60\n",
    "        ttl_days = ttl_hours // 24\n",
    "        print(f\"  - {measurement_type} del sensor {sensor_id} del día {date} (TTL: {ttl_seconds} segundos, {ttl_minutes} minutos, {ttl_hours} horas, {ttl_days} días)\")\n",
    "\n",
    "def test() -> None:\n",
    "    # Crear tablas\n",
    "    create_tables()\n",
    "    \n",
    "    # Generar datos de prueba\n",
    "    generate_sensor_data()\n",
    "    \n",
    "    # Ejecutar consultas de prueba\n",
    "    query_1('temperatura', 'SENS001')\n",
    "    query_2('humedad', 'SENS002')\n",
    "    query_3()\n",
    "    \n",
    "    # Limpiar tablas\n",
    "    drop_tables()\n",
    "\n",
    "test()\n",
    "# Cerrar conexión a Cassandra\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
