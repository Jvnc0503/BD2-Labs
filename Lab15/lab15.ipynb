{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f248124a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lab15\n",
    "\n",
    "## P1. (7 pts) Medir la temperatura y la humedad:\n",
    "### Consultas requeridas\n",
    "1. Obtener todas las mediciones de humedad del sensor 'SENS001' del último día. La consulta debe incluir el tiempo restante de vida (TTL) de cada registro.\n",
    "\n",
    "2. Detectar valores anómalos fuera del rango permitido en la última hora. Implementar una consulta que identifique mediciones de temperatura o humedad fuera del rango normal. La consulta debe permitir filtrar por sensor específico.\n",
    "\n",
    "3. Verificar el tiempo restante de vida de los datos usando la función TTL. Implementar una consulta que muestre el TTL en diferentes unidades (segundos, horas, días). Crear una consulta para identificar datos que están próximos a expirar (ej: en las próximas 24 horas).\n",
    "\n",
    "### Estructura de tablas propuestas\n",
    "La tabla *sensor_readings* tendrá un propósito general que permite filtrar por tipo de medicion, sensor y día. Esta tabla se usará para la consulta 1.\n",
    "\n",
    "La tabla *sensor_anomalies* será destinada a optimizar la consulta 2, esta nos permite filtrar por hora ya que se incluye este dato en el partition key, además de tener clustering por el valor de la medición lo que facilita hallar los valores anómalos.\n",
    "\n",
    "La tabla *sensor_by_date* fue pensada para usarse con la consulta 3, porque particiona solo por la fecha y con esto poder filtrar los datos con más de 6 días de antigüedad.\n",
    "\n",
    "Se respeta el tiempo de vida de los datos de 7 días de acuerdo a lo solicitado, y se considera adicionalmente un tiempo de vida de solo 2 horas para el seguimiento de datos anómalos ya que se espera que siempre se consulten los insertados en la hora pasada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811eb3c",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create table if not exists sensor_readings\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, date), event_time )\n",
    ") with clustering order by (event_time desc) and\n",
    "        default_time_to_live = 604800; -- 7 days\n",
    "\n",
    "create table if not exists sensor_anomalies\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    hour             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, hour), measurement, event_time)\n",
    ") with clustering order by (measurement asc, event_time desc) and\n",
    "        default_time_to_live = 7200; -- 2 hour\n",
    "\n",
    "create table if not exists sensor_by_date\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( date, event_time )\n",
    ") with clustering order by (event_time asc) and\n",
    "        default_time_to_live = 604800; -- 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e96cc4",
   "metadata": {},
   "source": [
    "### Configuración del entorno de trabajo\n",
    "\n",
    "Instalación de driver de cassandra para python, se requiere haber configurado un entorno local de conda previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58fbb15",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - anaconda\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - libev\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/anaconda\n",
      "  - defaults\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - msys2\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (3.29.2)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: click in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda update -n base -c defaults conda\n",
    "%conda install -c anaconda libev\n",
    "%conda install -c msys2 m2-make\n",
    "%conda install -c conda-forge pkg-config\n",
    "%pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b15792",
   "metadata": {},
   "source": [
    "### Generación de datos y pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserción de datos de prueba completada.\n",
      "Total de registros insertados: 100810\n",
      "\n",
      "Query 1:\n",
      "Lecturas de temperatura para el sensor SENS001 del día de ayer 2025-07-07:\n",
      "1440 resultados encontrados en 0.0445 segundos\n",
      "  - 2025-07-07 23:59:23: 29.201208291475858 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:58:23: 26.325772840598976 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:57:23: 16.36373289016412 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:56:23: 15.755248981596546 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:55:23: 29.982972045719457 (TTL: 604789 segundos)\n",
      "\n",
      "Query 2:\n",
      "Anomalías de humedad para el sensor SENS002 de la hora pasada 2025-07-08T15:\n",
      "20 resultados encontrados en 0.0318 segundos\n",
      "  - 2025-07-08 15:59:23: 26.105229674183267\n",
      "  - 2025-07-08 15:57:23: 84.11479523132168\n",
      "  - 2025-07-08 15:52:23: 29.90490526548601\n",
      "  - 2025-07-08 15:50:23: 29.105740757183067\n",
      "  - 2025-07-08 15:49:23: 26.49023812956321\n",
      "\n",
      "Query 3:\n",
      "Datos de más de 6 días de antigüedad (2025-07-02):\n",
      "1440 resultados encontrados en 0.0457 segundos\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement, BatchType\n",
    "\n",
    "# Parametros de simulación\n",
    "N_SENSORS = 5\n",
    "SAMPLING_RATE = 60 # 1 minuto en segundos\n",
    "SAMPLING_TIME = 604800  # 7 días en segundos\n",
    "ID_PREFIX = 'SENS'\n",
    "MESUREMENTS_TYPES = ('temperatura', 'humedad')\n",
    "NORMAL_RANGE = {'temperatura': (15, 35),\n",
    "                'humedad': (30, 80)}\n",
    "\n",
    "# Precomputar rango extendido de valores para simulación de anomalías\n",
    "EXTENDED_RANGE = {\n",
    "    type: (low - (high - low) * 0.2,\n",
    "        high + (high - low) * 0.2\n",
    "    )\n",
    "    for type, (low, high) in NORMAL_RANGE.items()\n",
    "}\n",
    "\n",
    "# Conexión a Cassandra en Docker\n",
    "cluster = Cluster(\n",
    "  ['localhost'], port=9042,\n",
    "  protocol_version=4,\n",
    "  connect_timeout=5,\n",
    "  idle_heartbeat_interval=30,\n",
    "  control_connection_timeout=10\n",
    ")\n",
    "cassandra = cluster.connect('my_keyspace')\n",
    "\n",
    "def create_tables() -> None:\n",
    "    # Crear tabla para lecturas de sensores\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_readings\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            date             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( (measurement_type, sensor_id, date), event_time )\n",
    "        ) with clustering order by (event_time desc) and\n",
    "                default_time_to_live = 604800\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_anomalies\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            hour             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( (measurement_type, sensor_id, hour), measurement, event_time)\n",
    "        ) with clustering order by (measurement asc, event_time desc) and\n",
    "                default_time_to_live = 7200\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_by_date\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            date             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( date, event_time )\n",
    "        ) with clustering order by (event_time asc) and\n",
    "                default_time_to_live = 604800;\n",
    "    \"\"\")\n",
    "\n",
    "def drop_tables() -> None:\n",
    "    # Eliminar tablas si existen\n",
    "    cassandra.execute(\"drop table if exists sensor_readings\")\n",
    "    cassandra.execute(\"drop table if exists sensor_anomalies\")\n",
    "    cassandra.execute(\"drop table if exists sensor_by_date\")\n",
    "    \n",
    "def generate_sensor_data() -> None:\n",
    "    # Generar indetificadores únicos para cada sensor\n",
    "    ids: list[str] = [f\"{ID_PREFIX}{str(i).zfill(3)}\" for i in range(1, N_SENSORS + 1)]\n",
    "\n",
    "    # Preparar queries\n",
    "    insert_reading = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_readings (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_anomaly = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_anomalies (measurement_type, sensor_id, hour, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_by_date = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_by_date (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    # Definir el tiempo de inicio y fin para la generación de datos\n",
    "    start_time: datetime = datetime.now() - timedelta(seconds=SAMPLING_TIME)\n",
    "    end_time: datetime = datetime.now()\n",
    "    current_time: datetime = end_time   # Insertar en orden descendente\n",
    "\n",
    "    # Usar BatchStatement para agrupar inserciones\n",
    "    batch = BatchStatement(batch_type=BatchType.UNLOGGED)\n",
    "\n",
    "    # Lista para almacenar las futuras ejecuciones asíncronas\n",
    "    futures = []\n",
    "\n",
    "    # Limite de inflight para evitar sobrecargar Cassandra\n",
    "    max_inflight = 16\n",
    "\n",
    "    while current_time >= start_time:\n",
    "        date = current_time.strftime('%Y-%m-%d')\n",
    "        hour = current_time.strftime('%Y-%m-%dT%H')\n",
    "\n",
    "        for id in ids:\n",
    "            for type in MESUREMENTS_TYPES:\n",
    "                ext_low, ext_high = EXTENDED_RANGE[type]\n",
    "                measurement = random.uniform(ext_low, ext_high)\n",
    "\n",
    "                batch.add(insert_reading, (type, id, date, current_time, measurement))\n",
    "                batch.add(insert_anomaly, (type, id, hour, current_time, measurement))\n",
    "                batch.add(insert_by_date, (type, id, date, current_time, measurement))\n",
    "\n",
    "        # Ejecutar el batch de forma asíncrona\n",
    "        futures.append(cassandra.execute_async(batch))\n",
    "        batch.clear()  # Limpiar el batch para la siguiente iteración\n",
    "\n",
    "        # Si el batch alcanza el límite de inflight, esperar a que se completen\n",
    "        if len(futures) >= max_inflight:\n",
    "            for f in futures:\n",
    "                f.result()\n",
    "            futures.clear()\n",
    "\n",
    "        # Retroceder el tiempo para la siguiente iteración\n",
    "        current_time -= timedelta(seconds=SAMPLING_RATE)\n",
    "    \n",
    "    # Esperar batchs restantes\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "    result = cassandra.execute(\"select count(*) from sensor_readings\")\n",
    "    count = list(result)[0][0]\n",
    "    print(\"Inserción de datos de prueba completada.\\nTotal de registros insertados:\", count)\n",
    "\n",
    "def query_1(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 1:\")\n",
    "    # Calcular el bucket del día anterior\n",
    "    target_date: str = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement, ttl(measurement) as ttl\n",
    "        from sensor_readings\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and date = %s\n",
    "    \"\"\", (type, id, target_date))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Lecturas de {type} para el sensor {id} del día de ayer {target_date}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        ttl_seconds = row.ttl\n",
    "        print(f\"  - {event_time}: {measurement} (TTL: {ttl_seconds} segundos)\")\n",
    "\n",
    "def query_2(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 2:\")\n",
    "    # Calcular el bucket de la hora anterior\n",
    "    target_hour: str = (datetime.now() - timedelta(hours=1)).strftime('%Y-%m-%dT%H')\n",
    "    # Obtener el rango normal para el tipo de medición\n",
    "    low, high = NORMAL_RANGE[type]\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows_low = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement < %s\n",
    "    \"\"\", (type, id, target_hour, low))\n",
    "\n",
    "    rows_high = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement > %s\n",
    "    \"\"\", (type, id, target_hour, high))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows_low) + list(rows_high)\n",
    "    rows.sort(key=lambda row: row.event_time, reverse=True)\n",
    "    print(f\"Anomalías de {type} para el sensor {id} de la hora pasada {target_hour}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 anomalías\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        print(f\"  - {event_time}: {measurement}\")\n",
    "\n",
    "def query_3() -> None:\n",
    "    print(\"\\nQuery 3:\")\n",
    "    # Calcular el bucket de la fecha de hace 6 días\n",
    "    target_date: str = (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select measurement_type, sensor_id, date, event_time, ttl(measurement) as ttl\n",
    "        from sensor_by_date\n",
    "        where date = %s\n",
    "    \"\"\", (target_date,))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Datos de más de 6 días de antigüedad ({target_date}):\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        measurement_type = row.measurement_type\n",
    "        sensor_id = row.sensor_id\n",
    "        date = row.date\n",
    "        ttl_seconds = row.ttl\n",
    "        ttl_minutes = ttl_seconds // 60\n",
    "        ttl_hours = ttl_minutes // 60\n",
    "        ttl_days = ttl_hours // 24\n",
    "        print(f\"  - {measurement_type} del sensor {sensor_id} del día {date} (TTL: {ttl_seconds} segundos, {ttl_minutes} minutos, {ttl_hours} horas, {ttl_days} días)\")\n",
    "\n",
    "def test() -> None:\n",
    "    # Crear tablas\n",
    "    create_tables()\n",
    "    \n",
    "    # Generar datos de prueba\n",
    "    generate_sensor_data()\n",
    "    \n",
    "    # Ejecutar consultas de prueba\n",
    "    query_1('temperatura', 'SENS001')\n",
    "    query_2('humedad', 'SENS002')\n",
    "    query_3()\n",
    "    \n",
    "    # Limpiar tablas\n",
    "    drop_tables()\n",
    "\n",
    "test()\n",
    "# Cerrar conexión a Cassandra\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7617a",
   "metadata": {},
   "source": [
    "## P2. (13 pts) Evaluación Experimental\n",
    "\n",
    "### Cluster de Cassandra con Docker Compose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6045af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: '3.8'\n",
      "\n",
      "services:\n",
      "  cassandra1:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra1\n",
      "    hostname: cassandra1\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    ports:\n",
      "      - \"9042:9042\"\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "  cassandra2:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra2\n",
      "    hostname: cassandra2\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    depends_on:\n",
      "      - cassandra1\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "  cassandra3:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra3\n",
      "    hostname: cassandra3\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    depends_on:\n",
      "      - cassandra1\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "networks:\n",
      "  cassandra-net:\n",
      "    driver: bridge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"docker-compose.yml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023c4461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container cassandra2  Stopping\n",
      " Container cassandra3  Stopping\n",
      " Container cassandra3  Stopped\n",
      " Container cassandra3  Removing\n",
      " Container cassandra3  Removed\n",
      " Container cassandra2  Stopped\n",
      " Container cassandra2  Removing\n",
      " Container cassandra2  Removed\n",
      " Container cassandra1  Stopping\n",
      " Container cassandra1  Stopped\n",
      " Container cassandra1  Removing\n",
      " Container cassandra1  Removed\n",
      " Network lab15_cassandra-net  Removing\n",
      " Network lab15_cassandra-net  Removed\n",
      " Network lab15_cassandra-net  Creating\n",
      " Network lab15_cassandra-net  Created\n",
      " Container cassandra1  Creating\n",
      " Container cassandra1  Created\n",
      " Container cassandra3  Creating\n",
      " Container cassandra2  Creating\n",
      " Container cassandra3  Created\n",
      " Container cassandra2  Created\n",
      " Container cassandra1  Starting\n",
      " Container cassandra1  Started\n",
      " Container cassandra2  Starting\n",
      " Container cassandra3  Starting\n",
      " Container cassandra3  Started\n",
      " Container cassandra2  Started\n"
     ]
    }
   ],
   "source": [
    "!docker compose down -v\n",
    "!docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bbafc",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE KEYSPACE IF NOT EXISTS my_keyspace\n",
    "    WITH replication = {\n",
    "        'class': 'NetworkTopologyStrategy',\n",
    "        'datacenter1': '3'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23c1af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace: my_keyspace\n",
      "Replicación: {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'datacenter1': '3'}\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Crear keyspace (si no existe)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS my_keyspace\n",
    "    WITH replication = {\n",
    "        'class': 'NetworkTopologyStrategy',\n",
    "        'datacenter1': '3'\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# Verificar la creación del keyspace\n",
    "rows = session.execute(\"\"\"\n",
    "    SELECT keyspace_name, replication\n",
    "    FROM system_schema.keyspaces\n",
    "    WHERE keyspace_name = 'my_keyspace'\n",
    "\"\"\")\n",
    "\n",
    "for row in rows:\n",
    "    print(\"Keyspace:\", row.keyspace_name)\n",
    "    print(\"Replicación:\", row.replication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407a946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS                                                       NAMES\n",
      "5674d14e8152   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp                 cassandra2\n",
      "3ba274a98a65   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp                 cassandra3\n",
      "c62be9070c46   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9160/tcp, 0.0.0.0:9042->9042/tcp   cassandra1\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2076ebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.21.0.4  75.44 KiB  16      100.0%            91278d84-a6f5-4c4a-a15d-4d130f69f9c3  rack1\n",
      "UN  172.21.0.2  75.45 KiB  16      100.0%            911df817-5bbb-4120-8416-5a526e0645ae  rack1\n",
      "UN  172.21.0.3  75.44 KiB  16      100.0%            7d2e0357-0bdc-4cfe-be43-3236e4121186  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker exec cassandra1 nodetool status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
