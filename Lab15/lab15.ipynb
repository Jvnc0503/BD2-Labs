{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f248124a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lab15\n",
    "\n",
    "## P1. (7 pts) Medir la temperatura y la humedad:\n",
    "### Consultas requeridas\n",
    "1. Obtener todas las mediciones de humedad del sensor `SENS001` del último día. La consulta debe incluir el tiempo restante de vida (TTL) de cada registro.\n",
    "\n",
    "2. Detectar valores anómalos fuera del rango permitido en la última hora. Implementar una consulta que identifique mediciones de temperatura o humedad fuera del rango normal. La consulta debe permitir filtrar por sensor específico.\n",
    "\n",
    "3. Verificar el tiempo restante de vida de los datos usando la función TTL. Implementar una consulta que muestre el TTL en diferentes unidades (segundos, horas, días). Crear una consulta para identificar datos que están próximos a expirar (ej: en las próximas 24 horas).\n",
    "\n",
    "### Estructura de tablas propuestas\n",
    "La tabla `sensor_readings` tendrá un propósito general que permite filtrar por tipo de medicion, sensor y día. Esta tabla se usará para la consulta 1.\n",
    "\n",
    "La tabla `sensor_anomalies` será destinada a optimizar la consulta 2, esta nos permite filtrar por hora ya que se incluye este dato en el partition key, además de tener clustering por el valor de la medición lo que facilita hallar los valores anómalos.\n",
    "\n",
    "La tabla `sensor_by_date` fue pensada para usarse con la consulta 3, porque particiona solo por la fecha y con esto poder filtrar los datos con más de 6 días de antigüedad.\n",
    "\n",
    "Se respeta el tiempo de vida de los datos de 7 días de acuerdo a lo solicitado, y se considera adicionalmente un tiempo de vida de solo 2 horas para el seguimiento de datos anómalos ya que se espera que siempre se consulten los insertados en la hora pasada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811eb3c",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create table if not exists sensor_readings\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, date), event_time )\n",
    ") with clustering order by (event_time desc) and\n",
    "        default_time_to_live = 604800; -- 7 days\n",
    "\n",
    "create table if not exists sensor_anomalies\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    hour             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( (measurement_type, sensor_id, hour), measurement, event_time)\n",
    ") with clustering order by (measurement asc, event_time desc) and\n",
    "        default_time_to_live = 7200; -- 2 hour\n",
    "\n",
    "create table if not exists sensor_by_date\n",
    "(\n",
    "    measurement_type text,\n",
    "    sensor_id        text,\n",
    "    date             text,\n",
    "    event_time       timestamp,\n",
    "    measurement      double,\n",
    "    primary key ( date, event_time )\n",
    ") with clustering order by (event_time asc) and\n",
    "        default_time_to_live = 604800; -- 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e96cc4",
   "metadata": {},
   "source": [
    "### Configuración del entorno de trabajo\n",
    "\n",
    "Instalación de driver de cassandra para python, se requiere haber configurado un entorno local de conda previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58fbb15",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - anaconda\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - libev\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/anaconda\n",
      "  - defaults\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - msys2\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (3.29.2)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: click in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jvnc\\documents\\bd2\\bd2-labs\\lab15\\.conda\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda update -n base -c defaults conda\n",
    "%conda install -c anaconda libev\n",
    "%conda install -c msys2 m2-make\n",
    "%conda install -c conda-forge pkg-config\n",
    "%pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b15792",
   "metadata": {},
   "source": [
    "### Generación de datos y pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserción de datos de prueba completada.\n",
      "Total de registros insertados: 100810\n",
      "\n",
      "Query 1:\n",
      "Lecturas de temperatura para el sensor SENS001 del día de ayer 2025-07-07:\n",
      "1440 resultados encontrados en 0.0445 segundos\n",
      "  - 2025-07-07 23:59:23: 29.201208291475858 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:58:23: 26.325772840598976 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:57:23: 16.36373289016412 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:56:23: 15.755248981596546 (TTL: 604789 segundos)\n",
      "  - 2025-07-07 23:55:23: 29.982972045719457 (TTL: 604789 segundos)\n",
      "\n",
      "Query 2:\n",
      "Anomalías de humedad para el sensor SENS002 de la hora pasada 2025-07-08T15:\n",
      "20 resultados encontrados en 0.0318 segundos\n",
      "  - 2025-07-08 15:59:23: 26.105229674183267\n",
      "  - 2025-07-08 15:57:23: 84.11479523132168\n",
      "  - 2025-07-08 15:52:23: 29.90490526548601\n",
      "  - 2025-07-08 15:50:23: 29.105740757183067\n",
      "  - 2025-07-08 15:49:23: 26.49023812956321\n",
      "\n",
      "Query 3:\n",
      "Datos de más de 6 días de antigüedad (2025-07-02):\n",
      "1440 resultados encontrados en 0.0457 segundos\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n",
      "  - temperatura del sensor SENS005 del día 2025-07-02 (TTL: 604799 segundos, 10079 minutos, 167 horas, 6 días)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement, BatchType\n",
    "\n",
    "# Parametros de simulación\n",
    "N_SENSORS = 5\n",
    "SAMPLING_RATE = 60 # 1 minuto en segundos\n",
    "SAMPLING_TIME = 604800  # 7 días en segundos\n",
    "ID_PREFIX = 'SENS'\n",
    "MESUREMENTS_TYPES = ('temperatura', 'humedad')\n",
    "NORMAL_RANGE = {'temperatura': (15, 35),\n",
    "                'humedad': (30, 80)}\n",
    "\n",
    "# Precomputar rango extendido de valores para simulación de anomalías\n",
    "EXTENDED_RANGE = {\n",
    "    type: (low - (high - low) * 0.2,\n",
    "        high + (high - low) * 0.2\n",
    "    )\n",
    "    for type, (low, high) in NORMAL_RANGE.items()\n",
    "}\n",
    "\n",
    "# Conexión a Cassandra en Docker\n",
    "cluster = Cluster(\n",
    "  ['localhost'], port=9042,\n",
    "  protocol_version=4,\n",
    "  connect_timeout=5,\n",
    "  idle_heartbeat_interval=30,\n",
    "  control_connection_timeout=10\n",
    ")\n",
    "cassandra = cluster.connect('my_keyspace')\n",
    "\n",
    "def create_tables() -> None:\n",
    "    # Crear tabla para lecturas de sensores\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_readings\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            date             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( (measurement_type, sensor_id, date), event_time )\n",
    "        ) with clustering order by (event_time desc) and\n",
    "                default_time_to_live = 604800\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_anomalies\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            hour             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( (measurement_type, sensor_id, hour), measurement, event_time)\n",
    "        ) with clustering order by (measurement asc, event_time desc) and\n",
    "                default_time_to_live = 7200\n",
    "    \"\"\")\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists sensor_by_date\n",
    "        (\n",
    "            measurement_type text,\n",
    "            sensor_id        text,\n",
    "            date             text,\n",
    "            event_time       timestamp,\n",
    "            measurement      double,\n",
    "            primary key ( date, event_time )\n",
    "        ) with clustering order by (event_time asc) and\n",
    "                default_time_to_live = 604800;\n",
    "    \"\"\")\n",
    "\n",
    "def drop_tables() -> None:\n",
    "    # Eliminar tablas si existen\n",
    "    cassandra.execute(\"drop table if exists sensor_readings\")\n",
    "    cassandra.execute(\"drop table if exists sensor_anomalies\")\n",
    "    cassandra.execute(\"drop table if exists sensor_by_date\")\n",
    "    \n",
    "def generate_sensor_data() -> None:\n",
    "    # Generar indetificadores únicos para cada sensor\n",
    "    ids: list[str] = [f\"{ID_PREFIX}{str(i).zfill(3)}\" for i in range(1, N_SENSORS + 1)]\n",
    "\n",
    "    # Preparar queries\n",
    "    insert_reading = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_readings (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_anomaly = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_anomalies (measurement_type, sensor_id, hour, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    insert_by_date = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO sensor_by_date (measurement_type, sensor_id, date, event_time, measurement)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    # Definir el tiempo de inicio y fin para la generación de datos\n",
    "    start_time: datetime = datetime.now() - timedelta(seconds=SAMPLING_TIME)\n",
    "    end_time: datetime = datetime.now()\n",
    "    current_time: datetime = end_time   # Insertar en orden descendente\n",
    "\n",
    "    # Usar BatchStatement para agrupar inserciones\n",
    "    batch = BatchStatement(batch_type=BatchType.UNLOGGED)\n",
    "\n",
    "    # Lista para almacenar las futuras ejecuciones asíncronas\n",
    "    futures = []\n",
    "\n",
    "    # Limite de inflight para evitar sobrecargar Cassandra\n",
    "    max_inflight = 16\n",
    "\n",
    "    while current_time >= start_time:\n",
    "        date = current_time.strftime('%Y-%m-%d')\n",
    "        hour = current_time.strftime('%Y-%m-%dT%H')\n",
    "\n",
    "        for id in ids:\n",
    "            for type in MESUREMENTS_TYPES:\n",
    "                ext_low, ext_high = EXTENDED_RANGE[type]\n",
    "                measurement = random.uniform(ext_low, ext_high)\n",
    "\n",
    "                batch.add(insert_reading, (type, id, date, current_time, measurement))\n",
    "                batch.add(insert_anomaly, (type, id, hour, current_time, measurement))\n",
    "                batch.add(insert_by_date, (type, id, date, current_time, measurement))\n",
    "\n",
    "        # Ejecutar el batch de forma asíncrona\n",
    "        futures.append(cassandra.execute_async(batch))\n",
    "        batch.clear()  # Limpiar el batch para la siguiente iteración\n",
    "\n",
    "        # Si el batch alcanza el límite de inflight, esperar a que se completen\n",
    "        if len(futures) >= max_inflight:\n",
    "            for f in futures:\n",
    "                f.result()\n",
    "            futures.clear()\n",
    "\n",
    "        # Retroceder el tiempo para la siguiente iteración\n",
    "        current_time -= timedelta(seconds=SAMPLING_RATE)\n",
    "    \n",
    "    # Esperar batchs restantes\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "    result = cassandra.execute(\"select count(*) from sensor_readings\")\n",
    "    count = list(result)[0][0]\n",
    "    print(\"Inserción de datos de prueba completada.\\nTotal de registros insertados:\", count)\n",
    "\n",
    "def query_1(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 1:\")\n",
    "    # Calcular el bucket del día anterior\n",
    "    target_date: str = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement, ttl(measurement) as ttl\n",
    "        from sensor_readings\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and date = %s\n",
    "    \"\"\", (type, id, target_date))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Lecturas de {type} para el sensor {id} del día de ayer {target_date}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        ttl_seconds = row.ttl\n",
    "        print(f\"  - {event_time}: {measurement} (TTL: {ttl_seconds} segundos)\")\n",
    "\n",
    "def query_2(type: str, id: str) -> None:\n",
    "    print(\"\\nQuery 2:\")\n",
    "    # Calcular el bucket de la hora anterior\n",
    "    target_hour: str = (datetime.now() - timedelta(hours=1)).strftime('%Y-%m-%dT%H')\n",
    "    # Obtener el rango normal para el tipo de medición\n",
    "    low, high = NORMAL_RANGE[type]\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows_low = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement < %s\n",
    "    \"\"\", (type, id, target_hour, low))\n",
    "\n",
    "    rows_high = cassandra.execute(\"\"\"\n",
    "        select event_time, measurement\n",
    "        from sensor_anomalies\n",
    "        where measurement_type = %s\n",
    "            and sensor_id = %s\n",
    "            and hour = %s\n",
    "            and measurement > %s\n",
    "    \"\"\", (type, id, target_hour, high))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows_low) + list(rows_high)\n",
    "    rows.sort(key=lambda row: row.event_time, reverse=True)\n",
    "    print(f\"Anomalías de {type} para el sensor {id} de la hora pasada {target_hour}:\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 anomalías\n",
    "        event_time = row.event_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        measurement = row.measurement\n",
    "        print(f\"  - {event_time}: {measurement}\")\n",
    "\n",
    "def query_3() -> None:\n",
    "    print(\"\\nQuery 3:\")\n",
    "    # Calcular el bucket de la fecha de hace 6 días\n",
    "    target_date: str = (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    # Ejecutar consulta y medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    rows = cassandra.execute(\"\"\"\n",
    "        select measurement_type, sensor_id, date, event_time, ttl(measurement) as ttl\n",
    "        from sensor_by_date\n",
    "        where date = %s\n",
    "    \"\"\", (target_date,))\n",
    "    end_time = time.time()\n",
    "    query_time = end_time - start_time\n",
    "    rows: list = list(rows)  # Convertir a lista para poder usar len() y slicing\n",
    "    print(f\"Datos de más de 6 días de antigüedad ({target_date}):\")\n",
    "    print(f\"{len(rows)} resultados encontrados en {query_time:.4f} segundos\")\n",
    "    for row in rows[:5]:  # Limitar a las primeras 5 lecturas\n",
    "        measurement_type = row.measurement_type\n",
    "        sensor_id = row.sensor_id\n",
    "        date = row.date\n",
    "        ttl_seconds = row.ttl\n",
    "        ttl_minutes = ttl_seconds // 60\n",
    "        ttl_hours = ttl_minutes // 60\n",
    "        ttl_days = ttl_hours // 24\n",
    "        print(f\"  - {measurement_type} del sensor {sensor_id} del día {date} (TTL: {ttl_seconds} segundos, {ttl_minutes} minutos, {ttl_hours} horas, {ttl_days} días)\")\n",
    "\n",
    "def test() -> None:\n",
    "    # Crear tablas\n",
    "    create_tables()\n",
    "    \n",
    "    # Generar datos de prueba\n",
    "    generate_sensor_data()\n",
    "    \n",
    "    # Ejecutar consultas de prueba\n",
    "    query_1('temperatura', 'SENS001')\n",
    "    query_2('humedad', 'SENS002')\n",
    "    query_3()\n",
    "    \n",
    "    # Limpiar tablas\n",
    "    drop_tables()\n",
    "\n",
    "test()\n",
    "# Cerrar conexión a Cassandra\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7617a",
   "metadata": {},
   "source": [
    "## P2. (13 pts) Evaluación Experimental\n",
    "\n",
    "### Cluster de Cassandra con Docker Compose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6045af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: '3.8'\n",
      "\n",
      "services:\n",
      "  cassandra1:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra1\n",
      "    hostname: cassandra1\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    ports:\n",
      "      - \"9042:9042\"\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "  cassandra2:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra2\n",
      "    hostname: cassandra2\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    depends_on:\n",
      "      - cassandra1\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "  cassandra3:\n",
      "    image: cassandra:4.1\n",
      "    container_name: cassandra3\n",
      "    hostname: cassandra3\n",
      "    networks:\n",
      "      - cassandra-net\n",
      "    depends_on:\n",
      "      - cassandra1\n",
      "    environment:\n",
      "      CASSANDRA_CLUSTER_NAME: \"CassandraCluster\"\n",
      "      CASSANDRA_DC: DC1\n",
      "      CASSANDRA_RACK: RAC1\n",
      "      CASSANDRA_SEEDS: \"cassandra1,cassandra2,cassandra3\"\n",
      "      MAX_HEAP_SIZE: 1024M\n",
      "      HEAP_NEWSIZE: 256M\n",
      "    mem_limit: 1536m\n",
      "\n",
      "networks:\n",
      "  cassandra-net:\n",
      "    driver: bridge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"docker-compose.yml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023c4461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container cassandra2  Stopping\n",
      " Container cassandra3  Stopping\n",
      " Container cassandra3  Stopped\n",
      " Container cassandra3  Removing\n",
      " Container cassandra3  Removed\n",
      " Container cassandra2  Stopped\n",
      " Container cassandra2  Removing\n",
      " Container cassandra2  Removed\n",
      " Container cassandra1  Stopping\n",
      " Container cassandra1  Stopped\n",
      " Container cassandra1  Removing\n",
      " Container cassandra1  Removed\n",
      " Network lab15_cassandra-net  Removing\n",
      " Network lab15_cassandra-net  Removed\n",
      " Network lab15_cassandra-net  Creating\n",
      " Network lab15_cassandra-net  Created\n",
      " Container cassandra1  Creating\n",
      " Container cassandra1  Created\n",
      " Container cassandra3  Creating\n",
      " Container cassandra2  Creating\n",
      " Container cassandra3  Created\n",
      " Container cassandra2  Created\n",
      " Container cassandra1  Starting\n",
      " Container cassandra1  Started\n",
      " Container cassandra2  Starting\n",
      " Container cassandra3  Starting\n",
      " Container cassandra3  Started\n",
      " Container cassandra2  Started\n"
     ]
    }
   ],
   "source": [
    "!docker compose down -v\n",
    "!docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bbafc",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE KEYSPACE IF NOT EXISTS my_keyspace\n",
    "    WITH replication = {\n",
    "        'class': 'NetworkTopologyStrategy',\n",
    "        'datacenter1': '3'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace: my_keyspace\n",
      "Replicación: {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'datacenter1': '3'}\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Crear keyspace (si no existe)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS my_keyspace\n",
    "    WITH replication = {\n",
    "        'class': 'NetworkTopologyStrategy',\n",
    "        'datacenter1': '3'\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# Verificar la creación del keyspace\n",
    "rows = session.execute(\"\"\"\n",
    "    SELECT keyspace_name, replication\n",
    "    FROM system_schema.keyspaces\n",
    "    WHERE keyspace_name = 'my_keyspace'\n",
    "\"\"\")\n",
    "\n",
    "for row in rows:\n",
    "    print(\"Keyspace:\", row.keyspace_name)\n",
    "    print(\"Replicación:\", row.replication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407a946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS                                                       NAMES\n",
      "5674d14e8152   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp                 cassandra2\n",
      "3ba274a98a65   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9042/tcp, 9160/tcp                 cassandra3\n",
      "c62be9070c46   cassandra:4.1   \"docker-entrypoint.s…\"   10 minutes ago   Up 10 minutes   7000-7001/tcp, 7199/tcp, 9160/tcp, 0.0.0.0:9042->9042/tcp   cassandra1\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2076ebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.21.0.4  75.44 KiB  16      100.0%            91278d84-a6f5-4c4a-a15d-4d130f69f9c3  rack1\n",
      "UN  172.21.0.2  75.45 KiB  16      100.0%            911df817-5bbb-4120-8416-5a526e0645ae  rack1\n",
      "UN  172.21.0.3  75.44 KiB  16      100.0%            7d2e0357-0bdc-4cfe-be43-3236e4121186  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker exec cassandra1 nodetool status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d5b8b",
   "metadata": {},
   "source": [
    "### PostgreSQL\n",
    "Se usa una instancia local en el puerto 5432."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2498b7da",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\Jvnc\\Documents\\BD2\\BD2-Labs\\Lab15\\.conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - asyncpg\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    asyncpg-0.27.0             |  py311ha68e1ae_1         570 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         570 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  asyncpg            conda-forge/win-64::asyncpg-0.27.0-py311ha68e1ae_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "asyncpg-0.27.0       | 570 KB    |            |   0% \n",
      "asyncpg-0.27.0       | 570 KB    | 2          |   3% \n",
      "asyncpg-0.27.0       | 570 KB    | #6         |  17% \n",
      "asyncpg-0.27.0       | 570 KB    | ###6       |  36% \n",
      "asyncpg-0.27.0       | 570 KB    | #######8   |  79% \n",
      "asyncpg-0.27.0       | 570 KB    | ########## | 100% \n",
      "asyncpg-0.27.0       | 570 KB    | ########## | 100% \n",
      "asyncpg-0.27.0       | 570 KB    | ########## | 100% \n",
      "                                                     \n",
      " done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge psycopg2\n",
    "%conda install -c conda-forge pandas\n",
    "%conda install -c conda-forge asyncpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c413f91",
   "metadata": {},
   "source": [
    "### Conexión desde Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4378aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "from datetime import datetime, timedelta\n",
    "from psycopg2.extras import execute_values\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster = Cluster(\n",
    "  ['localhost'], port=9042,\n",
    "  protocol_version=4,\n",
    "  connect_timeout=5,\n",
    "  idle_heartbeat_interval=30,\n",
    "  control_connection_timeout=10\n",
    ")\n",
    "cassandra = cluster.connect('my_keyspace')\n",
    "\n",
    "connect = psycopg2.connect(dbname=\"lab15\",user=\"postgres\",password=\"postgres\",host=\"localhost\",port=5432)\n",
    "connect.autocommit = True\n",
    "postgres = connect.cursor()\n",
    "\n",
    "def create_table_cassandra() -> None:\n",
    "    cassandra.execute(\"\"\"\n",
    "        create table if not exists temperature_measurements\n",
    "        (\n",
    "            sensor_id   text,\n",
    "            date        text,\n",
    "            event_time  timestamp,\n",
    "            temperature double,\n",
    "            humidity    double,\n",
    "            primary key ((sensor_id, date), event_time)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "def create_table_postgres() -> None:\n",
    "    postgres.execute(\"\"\"\n",
    "        create table if not exists temperature_measurements\n",
    "        (\n",
    "            sensor_id   varchar(20),\n",
    "            date        varchar(10),\n",
    "            event_time  timestamp,\n",
    "            temperature double precision,\n",
    "            humidity    double precision,\n",
    "            primary key (sensor_id, date, event_time)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "def drop_table_cassandra() -> None:\n",
    "    cassandra.execute(\"drop table if exists temperature_measurements\")\n",
    "\n",
    "def drop_table_postgres() -> None:\n",
    "    postgres.execute(\"drop table if exists temperature_measurements\")\n",
    "\n",
    "NORMAL_RANGE = [(15, 35), (30, 80)]  # Rango normal para temperatura y humedad\n",
    "EXTENDED_RANGE = [(low - (high - low) * 0.2, high + (high - low) * 0.2) for low, high in NORMAL_RANGE]\n",
    "\n",
    "def generate_data(sensors: int, days: int) -> list[tuple]:\n",
    "    now = datetime.now()\n",
    "    ids = [f\"SENS{str(i).zfill(3)}\" for i in range(1, sensors + 1)]\n",
    "    data: list[tuple] = []\n",
    "    for id in ids:\n",
    "        for day in range(days):\n",
    "            date_obj = now - timedelta(days=day)\n",
    "            date_str = date_obj.strftime('%Y-%m-%d')\n",
    "            for minute in range(24 * 60):\n",
    "                timestamp = date_obj.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(minutes=minute)\n",
    "                temperature = random.uniform(*EXTENDED_RANGE[0])\n",
    "                humidity = random.uniform(*EXTENDED_RANGE[1])\n",
    "                data.append((id, date_str, timestamp, temperature, humidity))\n",
    "    return data\n",
    "\n",
    "def insert_postgres(data: list[tuple]) -> float:\n",
    "    drop_table_postgres()\n",
    "    create_table_postgres()\n",
    "    query = \"\"\"\n",
    "        INSERT INTO temperature_measurements (sensor_id, date, event_time, temperature, humidity)\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    execute_values(postgres, query, data)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def insert_cassandra(data: list[tuple], batch_size: int) -> float:\n",
    "    drop_table_cassandra()\n",
    "    create_table_cassandra()\n",
    "    partitioned = defaultdict(list)\n",
    "    for row in data:\n",
    "        partitioned[(row[0], row[1])].append(row)\n",
    "    \n",
    "    prepared = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO temperature_measurements (sensor_id, date, event_time, temperature, humidity)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    \n",
    "    batch = BatchStatement()\n",
    "    start = time.time()\n",
    "    for rows in partitioned.values():\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            for row in rows[i:i + batch_size]:\n",
    "                batch.add(prepared, row)\n",
    "            cassandra.execute(batch)\n",
    "            batch.clear()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def insert_test() -> None:\n",
    "    print(\"Prueba de inserción de datos:\")\n",
    "    volumes: list = [7, 15, 30, 60] # Volúmenes de datos en días\n",
    "    batch_sizes: list = [100, 200, 500, 1000]\n",
    "    columns = ['dias', 'postgres'] + [f'cassandra({size})' for size in batch_sizes]\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    for volume in volumes:\n",
    "        data: list[tuple] = generate_data(5, volume)\n",
    "        row = {'dias': volume}\n",
    "        row['postgres'] = insert_postgres(data)\n",
    "        for size in batch_sizes:\n",
    "            row[f'cassandra({size})'] = insert_cassandra(data, size)\n",
    "        results = pd.concat([results, pd.DataFrame([row])], ignore_index=True)\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f9abb",
   "metadata": {},
   "source": [
    "### a) Pruebas de Escritura (INSERT):\n",
    "Se realizaron pruebas preliminares de inserción individual en Cassandra utilizando un volumen reducido de 5 sensores durante 7 días. El tiempo requerido para completar la inserción fue excesivo, por lo que se descartó esta estrategia y se optó por evaluar únicamente la inserción por lotes (batch).\n",
    "\n",
    "Para los experimentos se emplearon los siguientes parámetros:\n",
    "- Sensores: 5\n",
    "- Volúmenes de datos en días: 7, 15, 30, 60\n",
    "\n",
    "En el caso de PostgreSQL, el tamaño del batch corresponde siempre al total de datos generados para cada volumen, es decir, toda la inserción se realiza en una sola operación masiva. Para Cassandra, se evaluaron diferentes tamaños de batch (100, 200, 500 y 1000) para analizar su impacto en el rendimiento.\n",
    "\n",
    "La tabla de resultados presenta una columna para PostgreSQL y una columna para cada tamaño de batch en Cassandra, agrupando los resultados por volumen de datos (días).\n",
    "\n",
    "Este enfoque permite comparar de manera clara el efecto del tamaño de batch en Cassandra y la diferencia de desempeño respecto a PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa273997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba de inserción de datos:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba de inserción de datos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dias</th>\n",
       "      <th>postgres</th>\n",
       "      <th>cassandra(100)</th>\n",
       "      <th>cassandra(200)</th>\n",
       "      <th>cassandra(500)</th>\n",
       "      <th>cassandra(1000)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2.109822</td>\n",
       "      <td>30.822227</td>\n",
       "      <td>15.861103</td>\n",
       "      <td>4.538761</td>\n",
       "      <td>4.344583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>3.271147</td>\n",
       "      <td>66.552932</td>\n",
       "      <td>34.268575</td>\n",
       "      <td>15.353234</td>\n",
       "      <td>8.453861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>143.133282</td>\n",
       "      <td>130.770741</td>\n",
       "      <td>5650.858164</td>\n",
       "      <td>30.523972</td>\n",
       "      <td>17.589561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>13.444289</td>\n",
       "      <td>999.740117</td>\n",
       "      <td>509.043554</td>\n",
       "      <td>61.017116</td>\n",
       "      <td>36.388920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dias    postgres  cassandra(100)  cassandra(200)  cassandra(500)   \n",
       "0    7    2.109822       30.822227       15.861103        4.538761  \\\n",
       "1   15    3.271147       66.552932       34.268575       15.353234   \n",
       "2   30  143.133282      130.770741     5650.858164       30.523972   \n",
       "3   60   13.444289      999.740117      509.043554       61.017116   \n",
       "\n",
       "   cassandra(1000)  \n",
       "0         4.344583  \n",
       "1         8.453861  \n",
       "2        17.589561  \n",
       "3        36.388920  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insert_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00914db6",
   "metadata": {},
   "source": [
    "#### Propuesta de Optimización: Inserción Asíncrona y Concurrente\n",
    "\n",
    "Para mejorar el rendimiento en la inserción masiva de datos, se propone el uso de estrategias asíncronas y concurrentes tanto en Cassandra como en PostgreSQL:\n",
    "\n",
    "- **Cassandra:** Utilizar `execute_async` para enviar múltiples lotes (batches) en paralelo, controlando la cantidad de operaciones simultáneas con un parámetro de workers. Esto permite aprovechar mejor los recursos del clúster y reducir el tiempo total de inserción.\n",
    "\n",
    "- **PostgreSQL:** Implementar inserción concurrente usando `ThreadPoolExecutor` junto con `execute_values` (psycopg2), donde cada thread maneja su propio batch y conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bb26d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def insert_cassandra_async(data: list[tuple], batch_size: int, max_workers: int) -> float:\n",
    "    drop_table_cassandra()\n",
    "    create_table_cassandra()\n",
    "    partitioned = defaultdict(list)\n",
    "    for row in data:\n",
    "        partitioned[(row[0], row[1])].append(row)\n",
    "    prepared = cassandra.prepare(\"\"\"\n",
    "        INSERT INTO temperature_measurements (sensor_id, date, event_time, temperature, humidity)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    batches: list[BatchStatement] = []\n",
    "    for rows in partitioned.values():\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            batch = BatchStatement()\n",
    "            for row in rows[i:i + batch_size]:\n",
    "                batch.add(prepared, row)\n",
    "            batches.append(batch)\n",
    "    start = time.time()\n",
    "    futures = []\n",
    "    for batch in batches:\n",
    "        futures.append(cassandra.execute_async(batch))\n",
    "        if len(futures) >= max_workers:\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "            futures.clear()\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def insert_postgres_async(data: list[tuple], batch_size: int, max_workers: int) -> float:\n",
    "    drop_table_postgres()\n",
    "    create_table_postgres()\n",
    "    query = \"\"\"\n",
    "        insert into temperature_measurements (sensor_id, date, event_time, temperature, humidity)\n",
    "        values %s\n",
    "    \"\"\"\n",
    "    batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(execute_values, postgres, query, batch) for batch in batches]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def insert_test_async() -> None:\n",
    "    print(\"Prueba de inserción de datos batch + async:\")\n",
    "    volumes: list[int] = [7, 15, 30, 60] # Volúmenes de datos en días\n",
    "    workers_amounts: list[int] = [2, 4, 8, 16]   # Cantidad de workers para la inserción asíncrona\n",
    "    columns = ['dias'] + [f'cassandra({workers})' for workers in workers_amounts] + [f'postgres({workers})' for workers in workers_amounts]\n",
    "    results = pd.DataFrame(columns=columns, dtype=float)\n",
    "    for volume in volumes:\n",
    "        data: list[tuple] = generate_data(5, volume)\n",
    "        row = {'dias': float(volume)}\n",
    "        for workers in workers_amounts:\n",
    "            row[f'cassandra({workers})'] = insert_cassandra_async(data, 1000, workers)\n",
    "            row[f'postgres({workers})'] = insert_postgres_async(data, 1000, workers)\n",
    "        results = pd.concat([results, pd.DataFrame([row])], ignore_index=True)\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54364a",
   "metadata": {},
   "source": [
    "Se realizaron pruebas variando la cantidad de workers para analizar cómo escala el rendimiento de inserción en ambos motores.\n",
    "\n",
    "Los resultados muestran que **Cassandra** aprovecha mucho mejor el aumento de concurrencia: a mayor número de workers y mayor volumen de datos, el tiempo de inserción disminuye significativamente, llegando incluso a superar el rendimiento de PostgreSQL en escenarios de alta concurrencia y grandes volúmenes.\n",
    "\n",
    "Por el contrario, **PostgreSQL** no presenta mejoras notables al incrementar la cantidad de workers bajo este enfoque, ya que su modelo de concurrencia y manejo de conexiones limita el beneficio de la paralelización en la inserción por lotes. Esto resalta la arquitectura distribuida y orientada a la escalabilidad de Cassandra frente al enfoque tradicional de PostgreSQL para cargas masivas de escritura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da1e3d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba de inserción de datos batch + async:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dias</th>\n",
       "      <th>cassandra(2)</th>\n",
       "      <th>cassandra(4)</th>\n",
       "      <th>cassandra(8)</th>\n",
       "      <th>cassandra(16)</th>\n",
       "      <th>postgres(2)</th>\n",
       "      <th>postgres(4)</th>\n",
       "      <th>postgres(8)</th>\n",
       "      <th>postgres(16)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.350659</td>\n",
       "      <td>0.528152</td>\n",
       "      <td>0.374644</td>\n",
       "      <td>0.442715</td>\n",
       "      <td>1.152745</td>\n",
       "      <td>1.220498</td>\n",
       "      <td>1.146912</td>\n",
       "      <td>1.151179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.424133</td>\n",
       "      <td>0.980803</td>\n",
       "      <td>0.751128</td>\n",
       "      <td>0.654749</td>\n",
       "      <td>2.667092</td>\n",
       "      <td>2.705638</td>\n",
       "      <td>2.666091</td>\n",
       "      <td>2.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>6.270462</td>\n",
       "      <td>1.993381</td>\n",
       "      <td>1.564740</td>\n",
       "      <td>1.475055</td>\n",
       "      <td>5.539926</td>\n",
       "      <td>5.505820</td>\n",
       "      <td>5.416455</td>\n",
       "      <td>5.349141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>12.915060</td>\n",
       "      <td>3.903924</td>\n",
       "      <td>2.884975</td>\n",
       "      <td>2.543653</td>\n",
       "      <td>11.948052</td>\n",
       "      <td>11.177784</td>\n",
       "      <td>12.028290</td>\n",
       "      <td>13.132751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dias  cassandra(2)  cassandra(4)  cassandra(8)  cassandra(16)  postgres(2)   \n",
       "0   7.0      1.350659      0.528152      0.374644       0.442715     1.152745  \\\n",
       "1  15.0      3.424133      0.980803      0.751128       0.654749     2.667092   \n",
       "2  30.0      6.270462      1.993381      1.564740       1.475055     5.539926   \n",
       "3  60.0     12.915060      3.903924      2.884975       2.543653    11.948052   \n",
       "\n",
       "   postgres(4)  postgres(8)  postgres(16)  \n",
       "0     1.220498     1.146912      1.151179  \n",
       "1     2.705638     2.666091      2.746336  \n",
       "2     5.505820     5.416455      5.349141  \n",
       "3    11.177784    12.028290     13.132751  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insert_test_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc62cf",
   "metadata": {},
   "source": [
    "### b) Pruebas de Lectura (SELECT):\n",
    "1. Consulta por sensor y rango temporal: Obtener datos de un sensor específico en un día\n",
    "2. Consulta agregada: Calcular temperatura promedio de la última hora para múltiples sensores\n",
    "3. Consulta de rango de valores: Encontrar lecturas anómalas (fuera de rango normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a2388f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Lecturas del sensor SENS003 del día 2025-07-06:\n",
      "Cassandra (0.0507 segundos):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-06 23:59:00</td>\n",
       "      <td>18.806134</td>\n",
       "      <td>86.401322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-06 23:58:00</td>\n",
       "      <td>25.007504</td>\n",
       "      <td>37.339516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-06 23:57:00</td>\n",
       "      <td>16.905875</td>\n",
       "      <td>49.618386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-06 23:56:00</td>\n",
       "      <td>29.797197</td>\n",
       "      <td>77.201796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-06 23:55:00</td>\n",
       "      <td>31.299464</td>\n",
       "      <td>75.931630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2025-07-06 00:04:00</td>\n",
       "      <td>14.885724</td>\n",
       "      <td>87.427470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2025-07-06 00:03:00</td>\n",
       "      <td>30.369882</td>\n",
       "      <td>22.005966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2025-07-06 00:02:00</td>\n",
       "      <td>17.664961</td>\n",
       "      <td>54.864776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2025-07-06 00:01:00</td>\n",
       "      <td>30.681108</td>\n",
       "      <td>43.074342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2025-07-06 00:00:00</td>\n",
       "      <td>21.042819</td>\n",
       "      <td>54.021583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              event_time  temperature   humidity\n",
       "0    2025-07-06 23:59:00    18.806134  86.401322\n",
       "1    2025-07-06 23:58:00    25.007504  37.339516\n",
       "2    2025-07-06 23:57:00    16.905875  49.618386\n",
       "3    2025-07-06 23:56:00    29.797197  77.201796\n",
       "4    2025-07-06 23:55:00    31.299464  75.931630\n",
       "...                  ...          ...        ...\n",
       "1435 2025-07-06 00:04:00    14.885724  87.427470\n",
       "1436 2025-07-06 00:03:00    30.369882  22.005966\n",
       "1437 2025-07-06 00:02:00    17.664961  54.864776\n",
       "1438 2025-07-06 00:01:00    30.681108  43.074342\n",
       "1439 2025-07-06 00:00:00    21.042819  54.021583\n",
       "\n",
       "[1440 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL (0.0060 segundos):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-06 23:59:00</td>\n",
       "      <td>19.294152</td>\n",
       "      <td>49.198654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-06 23:58:00</td>\n",
       "      <td>31.928740</td>\n",
       "      <td>87.587672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-06 23:57:00</td>\n",
       "      <td>26.699335</td>\n",
       "      <td>51.608543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-06 23:56:00</td>\n",
       "      <td>28.003156</td>\n",
       "      <td>21.157938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-06 23:55:00</td>\n",
       "      <td>12.916244</td>\n",
       "      <td>57.648853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2025-07-06 00:04:00</td>\n",
       "      <td>31.667686</td>\n",
       "      <td>32.808141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2025-07-06 00:03:00</td>\n",
       "      <td>22.701466</td>\n",
       "      <td>46.347124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2025-07-06 00:02:00</td>\n",
       "      <td>30.496868</td>\n",
       "      <td>54.882868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2025-07-06 00:01:00</td>\n",
       "      <td>19.706237</td>\n",
       "      <td>49.527902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2025-07-06 00:00:00</td>\n",
       "      <td>13.292030</td>\n",
       "      <td>46.093424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              event_time  temperature   humidity\n",
       "0    2025-07-06 23:59:00    19.294152  49.198654\n",
       "1    2025-07-06 23:58:00    31.928740  87.587672\n",
       "2    2025-07-06 23:57:00    26.699335  51.608543\n",
       "3    2025-07-06 23:56:00    28.003156  21.157938\n",
       "4    2025-07-06 23:55:00    12.916244  57.648853\n",
       "...                  ...          ...        ...\n",
       "1435 2025-07-06 00:04:00    31.667686  32.808141\n",
       "1436 2025-07-06 00:03:00    22.701466  46.347124\n",
       "1437 2025-07-06 00:02:00    30.496868  54.882868\n",
       "1438 2025-07-06 00:01:00    19.706237  49.527902\n",
       "1439 2025-07-06 00:00:00    13.292030  46.093424\n",
       "\n",
       "[1440 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def query_1():\n",
    "    sensor_id = 'SENS003'\n",
    "    date = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')\n",
    "    print(f\"Query 1: Lecturas del sensor {sensor_id} del día {date}:\")\n",
    "\n",
    "    prepared = cassandra.prepare(\"\"\"\n",
    "        select event_time, temperature, humidity\n",
    "        from temperature_measurements\n",
    "        where sensor_id = ?\n",
    "        and date = ?\n",
    "        order by event_time desc\n",
    "    \"\"\")\n",
    "    start = time.time()\n",
    "    rows = cassandra.execute(prepared, (sensor_id, date))\n",
    "    end = time.time()\n",
    "    cassandra_time = end - start\n",
    "    print(f\"Cassandra ({cassandra_time:.4f} segundos):\")\n",
    "    display(pd.DataFrame(rows, columns=['event_time', 'temperature', 'humidity']))\n",
    "\n",
    "    start = time.time()\n",
    "    postgres.execute(\"\"\"\n",
    "        select event_time, temperature, humidity\n",
    "        from temperature_measurements\n",
    "        where sensor_id = %s\n",
    "        and date = %s\n",
    "        order by event_time desc\n",
    "    \"\"\", (sensor_id, date))\n",
    "    rows = postgres.fetchall()\n",
    "    end = time.time()\n",
    "    postgres_time = end - start\n",
    "    print(f\"PostgreSQL ({postgres_time:.4f} segundos):\")\n",
    "    display(pd.DataFrame(rows, columns=['event_time', 'temperature', 'humidity']))\n",
    "\n",
    "query_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
