{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE0Ev4ho2504"
   },
   "source": [
    "<div style=\"border-radius: 5px; padding: 1rem; margin-bottom: 1rem\">\n",
    "<img src=\"https://www.prototypesforhumanity.com/wp-content/uploads/2022/11/LOGO_UTEC_.png\" alt=\"Banner\" width=\"150\" />   \n",
    " </div>\n",
    "\n",
    "# Laboratorio 5.1: Procesamiento de Textos y Bag of Words\n",
    "\n",
    "> **Prof. Heider Sanchez**  \n",
    "> **ACLs:** Ana María Accilio, Sebastián Loza\n",
    "\n",
    "##  Introducción\n",
    "Este laboratorio tiene como objetivo el análisis y búsqueda de documentos textuales utilizando procesamiento de lenguaje natural (NLP) y una base de datos PostgreSQL. Se trabajará paso a paso desde la extracción de los textos hasta la aplicación búsquedas booleanas.\n",
    "\n",
    "\n",
    "### Objetivos\n",
    "- Configurar la tabla en PostgreSQL y carga de datos.\n",
    "- Desde Python leer los textos desde PostgreSQL.\n",
    "- Realizar el procesamiento de textos: convertir a minúscula, tokenización, stopwords, stemming y frecuencia de términos.\n",
    "- Almacenar los Bag of Words en la base de datos en formato JSON.\n",
    "- Realizar búsquedas de documentos similares a una consulta booleana (conectores AND, OR y AND-NOT).\n",
    "\n",
    "\n",
    "### Requisitos previos\n",
    "\n",
    "- Tener instalado PostgreSQL en su computadora (ultima versión)\n",
    "- Tener instalado las siguientes dependencias en Python:\n",
    "\n",
    "    ```bash\n",
    "    pip install psycopg2-binary nltk scikit-learn pandas\n",
    "    ```\n",
    "\n",
    "- Opcionalmente descargar los recursos de NLTK:\n",
    "\n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (2 puntos) Configurar la tabla en PostgreSQL y carga de datos\n",
    "\n",
    "\n",
    "### Crear las tablas\n",
    "\n",
    "Crear la tabla en PostgreSQL para almacenar los textos de noticias y el bag of words:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE noticias (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    url TEXT,\n",
    "    contenido TEXT,\n",
    "    categoria VARCHAR(50),\n",
    "    bag_of_words JSONB\n",
    ");\n",
    "```\n",
    "\n",
    "Además, crear una tabla para almacenar los stopwords\n",
    "\n",
    "```sql\n",
    "CREATE TABLE stopwords (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    word TEXT UNIQUE NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "### Carga de datos en PostgreSQL\n",
    "\n",
    "Proceder a cargar el dataset de noticias `news_es.csv` y el dataset de stopwords `stoplist_es.txt`.\n",
    "\n",
    "### Leer desde PostgreSQL con Python\n",
    "\n",
    "Completar la función para conectarte a PostgreSQL y leer los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def connect_db():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"<DB>\",\n",
    "        user=\"<USER>\",\n",
    "        password=\"<PASSWORD>\",\n",
    "        host=\"<HOST>\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "def fetch_data():\n",
    "    conn = connect_db()\n",
    "    query = \"SELECT id, contenido FROM noticias;\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias_df = fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (4 puntos) Preprocesamiento de texto\n",
    "\n",
    "Implementar la función `preprocess` que reciba un texto y realice los siguiente:\n",
    "- Convertir el texto a minuscula.\n",
    "- Tokenización.\n",
    "- Eliminación de stopwords\n",
    "- Stemming (raíz de las palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Implementar la función de preprocesamiento aquí\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, implementar una función para calcular la frecuencia de términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(text):\n",
    "    # Implementar la función de cálculo de BOW aquí\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (3 puntos) Actualizar la base de datos con los Bag of Words\n",
    "\n",
    "Guardar el resultado del Bag of Words en la columna `bag_of_words` de la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bow_in_db(dataframe):\n",
    "    # Implementar la función de actualización en la base de datos aquí\n",
    "    pass\n",
    "\n",
    "update_bow_in_db(noticias_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (7 puntos) Consulta booleana con filtrado por keywords\n",
    "\n",
    "Antes de aplicar el filtrado desde Python, es importante entender cómo funciona la consulta de una clave dentro de una columna JSONB en PostgreSQL. \n",
    "\n",
    "### Ejemplo consulta SQL en JSON:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM noticias WHERE bag_of_words ? 'keyword';\n",
    "```\n",
    "\n",
    "Esta consulta selecciona todos los registros en los que el `bag_of_words` (formato JSONB) contiene una clave igual a `'keyword'`. El operador `?` verifica la existencia de una clave dentro de un JSON.\n",
    "\n",
    "### Consulta booleana \n",
    "Implementar una función que permita parsear una consulta textual con conectores AND, OR y AND-NOT y con ello se aplique el filtro correspondiente directamente desde la base de datos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_boolean_query(query):\n",
    "    # Construir la condición de búsqueda a partir de la query booleana\n",
    "    # Ejecutar la consulta en la base de datos\n",
    "    # Retornar un DataFrame con los resultados\n",
    "    return pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas funcionales\n",
    "\n",
    "Realizar al menos 8 pruebas funcionales con mas de dos keywords de consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"transformación AND sostenible\", # Consulta con AND\n",
    "    \"México OR Perú\",  # Consulta con OR\n",
    "    \"México AND-NOT Perú\",  # Consulta con AND-NOT\n",
    "    \"nonexistent term\",  # no debería devolver resultados\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Probando consulta: '{query}'\")\n",
    "    results = apply_boolean_query(query)\n",
    "\n",
    "    if results.empty:\n",
    "        print(\"No se encontraron documentos.\")\n",
    "    else:\n",
    "        print(\"Resultados encontrados:\")\n",
    "        print(results[['id', 'text_column']].head())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (4 puntos) Actividad Final\n",
    "- Incluir lematización en el preprocesamiento de texto.\n",
    "- Medir el tiempo de ejecución de las consultas con diferentes tamaños de datos y optimizar el código según sea necesario.\n",
    "- Implementar una función para exportar los resultados de las consultas a un archivo CSV o JSON para su análisis posterior.\n",
    "\n",
    "\n",
    "**Entregable:** informe de los resultados obtenidos en formato PDF."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMD3xCd43oYqpOj0D0zRkpv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
